Notes mostly based on \citet{impaproba}, \citet{sokol2013advanced}
and the course Probability II at IMPA ministered in 2020 by professor
Augusto Teixeira.
Most introductions to Probability focuse on $\mathbb R$. Thus,
I call this ``Advanced Probability'', because we'll present
the theory in Metric Spaces.

\section{Probability Basics}

\subsection{Defining a Probability Measure}

\begin{definition}[$\sigma$-Algebra]
	We say that $\mathcal F \subset \mathcal P(\Omega)$ is a $\sigma$-algebra
	if
	\begin{enumerate}[(i)]
		\item $\Omega \in \mathcal F$,
		\item $A \in \mathcal F \implies A^c \in \mathcal F$,
		\item if $A_n \in \mathcal F$ for all $n \in \mathbb N \implies
			      \cup_{n\in \mathbb N} A_n \in \mathcal F$.
	\end{enumerate}
\end{definition}

\begin{definition}[Borel $\sigma$-algebra]
	The Borel $\sigma$-algebra of $\mathbb R$ is the one generated by the family of all open sets
	on $\mathbb R$. Let $U$ be the set of all open sets, $\Lambda := \{\mathcal G : U \subset \mathcal G\}$ is
	the family of all $\sigma$-algebras that contains all open sets, hence
	\begin{equation}
		\mathcal B := \bigcap_{\mathcal F_i \in \Lambda} \mathcal F_i
	\end{equation}
\end{definition}

\begin{definition}[Measurable space]
	We call $(\Omega, \mathcal F)$ a measurable space, where $\mathcal F$ is a $\sigma$-algebra of $\Omega$.
\end{definition}

\begin{definition}[Measure]
	We call $\mu$ a (positive) measure in the measurable space $(\Omega, \mathcal F)$
	if $\mu: \mathcal F \to [0,+\infty]$ and $\mu$ is \textit{countably-additive}, i.e.
	for any disjoint countable collection of sets $A_n \in \mathcal F$, we have

	\begin{equation}
		\mu\left(\bigcup_{n=1}^{+\infty}A_n\right)=
		\mu\left(\sum_{n=1}^{+\infty}A_n\right) = \sum_{n\in \mathbb N} \mu(A_n).
	\end{equation}
\end{definition}

\begin{definition}[Probability Measure]
	A measure $\mu$ defined on $(\Omega, \mathcal F)$ is a probability measure if
	\begin{equation}
		\mu(\Omega) = 1.
	\end{equation}
\end{definition}

\begin{definition}[Probability Space]
	We call the triple $(\Omega, \mathcal F, P)$ a probability space if $P$ is
	a probability measure $P: \mathcal F \to [0,1]$, and $\mathcal F$ is a
	$\sigma$-algebra on $\Omega$.
\end{definition}

\begin{definition}[Space of Probabilities]
	We call $\mathcal P(E)$ the space of probability measures defined
	on $(E,\mathcal F)$, where the $\sigma$-algebra $\mathcal F$.
	Let $(E,d)$ be a metric space. Whenever we write $\mathcal P(E)$
	without specifying more about the underlying space $E$, then
	we are assuming that we are using the topology induced by $d$
	and that we are working on the Borel $\sigma$-algebra
	of such induced topology.
\end{definition}

\begin{proposition}[Basic properties of probability measures]
	Let $(\Omega, \mathcal F, P)$ be a probability space. The following affirmatives are true:
	\begin{enumerate}[(i)]
		\item (\textbf{Complement}). $P(A) = 1 - P(A^c)$;
		\item (\textbf{Monotonicity}). If $A \subset B$, then $P(A) \leq P(B)$;
		\item (\textbf{Subadditivity}). $P(\cup^{+\infty}_{n=1}A_n) \leq \sum^{+\infty}_{n=1}P(A_n)$;
		\item (\textbf{Continuity from below}). If $A_n \uparrow A$,
		      then $\lim_n P(A_n)=P(A)$;
		\item (\textbf{Continuity from above}). If $A_n \downarrow A$,
		      then $\lim_n P(A_n)=P(A)$;
		\item (\textbf{Continuity}). If $A_n \to A$,
		      then $\lim_n P(A_n)=P(A)$.
	\end{enumerate}
\end{proposition}
\begin{prf}

	(i)
	\begin{equation*}
		1=P(\Omega) = P(A \cup A^c) = P(A + A^c) = P(A) + P(A^c).
	\end{equation*}

	(ii)
	\begin{equation*}
		B\cap A = A, \ B = (B\setminus A)+ (B \cap A) =
		(B\setminus A)+ A \implies P(B) = P(B\setminus A) + P(A) \implies P(B) \geq P(A).
	\end{equation*}

	(iii) Let $B_1 = A_1, B_2 = A_2 \setminus A_1, B_3 = A_3 \setminus(A_1 \cup A_2) ...$ which implies
	that $B_1 + B_2 + ... = A$. Hence,
	\begin{equation*}
		P(A) = P(\sum_{n=1}^{+\infty} B_n) =
		\sum_{n=1}^{+\infty} P(B_n) =
		\lim_{k\to +\infty}\sum_{n=1}^{k} P(B_n) = \lim_{k\to+\infty} P(A_k).
	\end{equation*}
	(iv) Note that $A_1 \supset A_2 \implies A_1^c \subset A_2^c$, thus
	$A_n \downarrow A$ implies $A_n^c \uparrow A^c$. Now use the same argument as (iii)
	to show that $\lim P(A_n^c) = P(A^c) = 1 - P(A) = \lim 1 - P(A_n) \implies P(A) = \lim P(A_n)$.

	\vspace{1cm}
	(v) If $A_n \to A$, then for $B_k = \cup_{n \geq k} A_n$, we have $B_k \supset B_{k+1}$ and
	\begin{equation*}
		\lim A_n = \limsup A_n = \lim_{k\to +\infty}\bigcup_{n\geq k}A_n
		=\bigcap_{k=1}^{+\infty}\bigcup_{n\geq k}A_n
		=\bigcap_{k=1}^{+\infty}B_k \implies B_k \downarrow A.
	\end{equation*}
	Hence, $\lim_k P(B_k) = P(A)$ and $P(B_k) \geq P(A_n)$ for every $n \geq k$. Thus,
	$\sup_{n\geq k}P(A_n) \leq P(B_k) \implies \limsup_k P(A_k) \leq \lim P(B_k) = P(\limsup A_n)= P(A)$.
	Now, make $C_k = \bigcap_{n\geq k}A_n$, then
	\begin{equation*}
		\lim A_n = \liminf A_n = \lim_{k\to +\infty}\bigcap_{n\geq k}A_n
		=\bigcup_{k=1}^{+\infty}\bigcap_{n\geq k}A_n
		=\bigcup_{k=1}^{+\infty}C_k \implies C_k \uparrow A.
	\end{equation*}
	Hence, $\lim_k P(C_k) = P(A)$ and $P(C_k) \leq P(A_n)$ for every $n \geq k$. Thus,
	$\inf_{n \geq k}P(A_n)\geq P(C_k) \implies \liminf_k P(A_k) \geq \lim P(C_k)=P(A)$.

	Finally, $\liminf P(A_n) \geq P(A) \geq \limsup P(A_n) \implies P(A_n) \to P(A)$.

\end{prf}

It might seem odd why most books state the properties of continuity from below and
from above instead of going straight to continuity (vi). The reason for this is that
one can actually show that $\sigma$-additivity is actually equivalent to
finite-additivity with continuity from either below or above.

\begin{proposition}
  Let $P$ be a measure that is finitely additive, i.e.
  for disjoint sets $A_1,...,A_n$,
  \begin{displaymath}
    P(\cup_{i=1}^n A_i) = \sum^n_{i=1} P(A_i).
  \end{displaymath}
  And $P$ is continuous from below (above).
  This implies that $P$ is $\sigma$-additive.
\end{proposition}
\begin{proof}
  Let $A = \cup_{i=1}^\infty A_i$ where $A_1,...$ are disjoint. We want to show that indeed
  $P(A) = \sum^\infty_{i=1}P(A_i)$. In order to do this, consider the sets
  $B_n:= A \setminus \cup^n_{i=1}A_i$. It's clear that $B_n \downarrow \varnothing$
  and $A_i \cap B_i = \varnothing$ for every $i \in \mathbb N$. Thus,
  \begin{align*}
    P(A) = P(\cup^n_{i=1} A_i \cup B_n) = \sum^n_{i=1} P(A_i) + P(B_n) \\ \implies
    P(A) =
    \lim_{n\to \infty} \sum^n_{i=1} P(A_i) + P(B_n).
  \end{align*}
  Since everything is positive, the limit of the sum is the sum of the limits, i.e.
  $\lim_{n\to \infty} \sum^n_{i=1} P(A_i) + P(B_n) =
  \lim_{n\to \infty} \sum^n_{i=1} P(A_i) + \lim_{n \to \infty} P(B_n)$. Since
  $\lim_{n\to \infty} P(B_n) = 0$, we conclude the proof.

  For the case of above continuity, just make $B_n = \cup^n_{i=1} A_i$ and
  hence $P(B_n) = \sum^n_{i=1}P(A_i) \uparrow P(A)$.

\end{proof}

\subsection{Random Variables}

\begin{definition}[Random Variable]

	Let $(\Omega, \mathcal F, P)$ be a probability space, and $(E,\mathcal E)$ be a measurable space
	(i.e. $\mathcal B$ is a $\sigma$-algebra of $E$). A function $X:\Omega\to E$ is called
	a random variable if $X$ is measurable, i.e.
	\begin{equation}
		X^{-1}(B) = \{\omega: X(\omega) \in B\} \in \mathcal F, \quad \forall B \in \mathcal E.
	\end{equation}
	Note that the notion of measurability is related to the spaces on which the function is acting.
\end{definition}

It's common to define random variables from $(\Omega, \mathcal F, P)$ to
$(\mathbb R, \mathcal B)$, where $\mathcal B$ is the Borel $\sigma$-algebra.

Once we have a random variable (which is nothing more than a function),
we can construct an image probability measure, which is also
sometimes called the random variable's distribution. This is done
via the pushforward operation which is formalized below.

\begin{definition}[Pushforward]
	Let $(E,\mathcal F)$ and $(E', \mathcal G)$ be measurable spaces, $h : E \to E'$ a measurable map
	and $P \in \mathcal P(E)$. We call $h_\# P$ the
	pushforward of $h$ by $P$, where:
	\begin{equation}
		h_\#P(B) = P(h^{-1}(B)),\quad \forall B \in \mathcal G.
	\end{equation}
\end{definition}

\begin{theorem}[Change of Variable]
	Let $T: E \to E'$ be a measurable map between
	$(E, \mathcal F, P)$ and $(E', \mathcal G)$. Then,
	$T_\# P$ is a probability measure on $(E', \mathcal G)$ and
	$\forall f$ measurable and integrable with respect to
	$T_\#P$ one has:
	\begin{equation}
		\int_{E'} f dT_\#P = \int_E f \circ T dP
	\end{equation}
	\label{thm:pushforward}
\end{theorem}
\begin{proof}
	Let $f_n$ be a simple positive measurable function. Hence
	\begin{equation*}
		\begin{multlined}
			f_n(y) = \sum^N_{i=0} a_i \mathbbm 1_{A_i}(y) \ \therefore
			\int_{E'} f_n \ dT_\# P =
			\sum^N_{i=0} a_i T_\# P(A_i) =
			\sum^N_{i=0} a_i P(T^{-1}(A_i) )
			\\
			(f_n\circ T)(x) =
			\sum^N_{i=0} a_i \mathbbm 1_{A_i}(T(x))=
			\sum^N_{i=0} a_i \mathbbm 1_{T^{-1}(A_i)}(x)
			\\
			\therefore
			\\
			\int_E f_n \circ T \ dP =
			\sum^N_{i=0} a_i P(T^{-1}(A_i) ) \\
		\end{multlined}
	\end{equation*}

	Hence, $\int_E f_n \circ T \ dP = \int_{E'} f_n \ dT_\# P$.

	Now, for a positive integrable measurable function
	$f$, there exists a sequence
	of positive simple functions such that $f_n \uparrow f$. Then,
	by the Monotone Convergence Theorem,
	\begin{align*}
		\int_{E'} f \ dT_\# P =
		\int_{E'}\lim_{n\to +\infty}  f_n \ dT_\# P & =
		\lim_{n\to +\infty} \int_{E'} f_n \ dT_\# P =                                               \\
		                                            & =\lim_{n\to +\infty}	\int_E f_n \circ T \ dP =
		\int_{E'} f \ dT_\# P
	\end{align*}

	If $f$ is non-positive, just use the same argument by splitting
	the negative and positive portions of the function.

\end{proof}


\begin{definition}[Regular Probability]
	Let $(E,\mathcal B(E), P)$ be a metric space with the Borel $\sigma$-algebra
	and a probability measure $P$.
	We say that such probability measure is regular if
	for every $\varepsilon>0$ and $A \in \mathcal B$, there is a
	closed set $F$ and an open set $G$ such that
	$F\subset A \subset G$ and $P(G \setminus F)< \varepsilon$.
\end{definition}

\begin{theorem}[Regular Metric Spaces]
	Every metric space $(E, \mathcal B(E), P)$ is regular,
	where $\mathcal B(E)$ is the Borel $\sigma$-algebra
	for the metric space $E$. Remember that every
	metric space has an induced topology, which
	we'll use to characterize the Borel $\sigma$-algebra.
\end{theorem}

\begin{proof}
	For a closed set $A$, make $F=A$.
	Let
	\begin{displaymath}
		G^\delta = B(A,\delta) = \{
		x \in E: d(x,A) < \delta
		\}.
	\end{displaymath}
	Note that $G^\delta$ is open, since $G^\delta  = \cup_{x \in A} B(x,\delta)$ is the union of open sets.
	Since $G^\delta \downarrow A$, then $P(G^\delta) \to P(A)$.

	Now, let's prove for the case hat $A$ is not close.
	Define
	\begin{displaymath}
		\mathcal G := \{
		A \in \mathcal B(E): \forall \varepsilon >0 \exists
		F \subset A \subset G \ : G \text{ is open and }
		F \text{ is closed}
		\}
	\end{displaymath}
	We just proved above that every closed set is in $\mathcal G$.
	Note that $\mathcal G \subset \mathcal B(E)$, since
	every element of $\mathcal G$ is by definition in
	$\mathcal B(E)$. But, if we can prove that
	$\mathcal G$ is a $\sigma$-algebra, then
	since $\mathcal G$ contains all the closed sets,
	then $\mathcal B \subset \mathcal G$, which
	would imply that $\mathcal B = \mathcal G$
	\footnote{This is a useful technique in Measure Theory,
		where we construct a set implicitly with the property
		we want to prove.}.

	Let's show that $\mathcal G$ is a $\sigma$-algebra.
	\begin{enumerate}[(i)]
		\item $\Omega \in \mathcal G$ since it's closed;
		\item For $A \in \mathcal G$, for every $\varepsilon$
		      we know that $F \subset A \subset G$ and
		      $G^c \subset A^c \subset F^c$ and
		      $P(F^c \setminus G^c) = P(F^c \cap G)= P(G \setminus F) <\varepsilon$, hence, $A^c \in \mathcal G$;
		\item $A_1,A_2... \in \mathcal G$. For every
		      $A_n$, there is $F_n, G_n$, such that
		      $P(G_n \setminus F_n)< \frac{2^{-n}\varepsilon}{2}$.
		      Since $\cup_{i=1}^n F_i \uparrow \cup^\infty_{i=1}F_i$,
		      there is $\cup^N_{i=1}F_i$ closed,
		      such that $P(\cup^\infty_{i=1} F_i\setminus
			      \cup^N_{i=1}F_i) < \varepsilon/2$.
		      $\cup^N_{i=1}F_i \subset \cup A_i \subset
			      \cup^\infty_{i=1}G_i = G$. Finally,
		      $P(G\setminus \cup^N_{i=1}F_i)\leq
			      P(G\cup^\infty_{i=1}F_i)+
			      P(\cup^\infty_{i=1}F_i \setminus
			      \cup^N_{i=1}F_i) \leq
			      \sum_{i=1}^\infty P(G_i \setminus F_i)
			      + \varepsilon/2 \leq
			      \sum^\infty_{i=1}\frac{2^{-i}\varepsilon}{2}
			      + \varepsilon/2 = \varepsilon.
		      $
	\end{enumerate}
	Thus we conclude the proof.

\end{proof}

\begin{corollary}
	Let $(E, \mathcal B(E), P)$ and $(E, \mathcal B(E), Q)$ be probability
	spaces such that $P(F)= Q(F), \forall F$ closed.
	Then, $P(A) = Q(A), \forall A \in \mathcal B(E)$, i.e. $P=Q$.
\end{corollary}
\begin{proof}
	We can approximate any measurable set $A$ by a closed set $F$.
	And since probability measures are continuous, we have
	that
	\begin{displaymath}
		F_n \uparrow A \implies \lim_{n\to \infty}
		P(F_n) = \lim_{n\to \infty} Q(F_n) = P(A) = Q(A).
	\end{displaymath}

\end{proof}

\begin{theorem}
	Let $P, Q$ be probability measures defined on $(E,\mathcal B(E))$ and
	$\int f dP = \int f dQ, \forall f:E \to \mathbb R$
	continuous and bounded (i.e $ f \in C_b(E)$).
	Then, $P = Q$.
\end{theorem}
\begin{proof}
	Let $F$ be a closed set. Next, define
	\begin{displaymath}
		f(x) := \left(
		1 - \frac{d(x,F)}{\varepsilon}
		\right)_+ =
		\max\left(0,
		1 - \frac{d(x,F)}{\varepsilon}
		\right).
	\end{displaymath}
	This function is continuous and bounded by $[0,1]$.

	But,$\mathbbm 1_F(x) \leq f(x) \leq \mathbbm 1_{B_\varepsilon(F)}(x)$, thus
	\begin{align*}
		P(F) \leq \int f dP = \int f dQ \leq Q(B_{\varepsilon}(F)).
	\end{align*}
	Since $B_\varepsilon (F)\downarrow F$, then
	$\lim_{\varepsilon \to 0 } Q(B_{\varepsilon}(F)) = Q(F)$.
  We can make the same argument for $P$ instead of $Q$, thus,
  we conclude that $P=Q$.

\end{proof}

\subsection{Weak Convergence of Probability Measures}

\begin{definition}[Weak Convergence for Probability]
	Let $(E, \mathcal B(E))$. We say that
	$P_n \rightharpoonup P$ (i.e. $P_n$ converges weakly to $P$) if
	$\forall f \in C_b(E)$ we have
	\begin{displaymath}
		\int f dP_n \to_n \int f dP.
	\end{displaymath}
\end{definition}

\begin{definition}[Continuity Set]
	$A \in \mathcal B(E)$ is called a continuity set of $E$
	if $P(\partial A) = 0$ (remember that $\partial A$ is the boundary of $A$, i.e.
	$\partial A := \bar A \setminus \mathring A$).
\end{definition}

\begin{theorem}[Theorem of Portmanteau]
	Consider $(E, \mathcal B(E))$.
	The following statements are equivalent:
	\begin{enumerate}[(i)]
		\item $P_n \rightharpoonup P$;
		\item $\forall f$ uniformly continuous and bounded, then $\int f dP_n \to \int f dP$;
		\item $\forall F$ closed, then $\limsup_n P_n(F)\leq P(F)$;
		\item $\forall G$ open, then $\liminf P_n(G)\geq P(G)$;
		\item $P_n(A) \to P(A), \forall A$ in the continuity set of $E$.
	\end{enumerate}
	\label{thm:portmanteau}
\end{theorem}

\begin{proof}

	[\textbf{(i)}$\implies$ \textbf{(ii)}]
	This is clear, since by definition of weak convergence
	we have that $\int f dP_n \to \int f dP$ for every $f \in C_b$, thus,
	the same is true if $f$ is $C_b$ and uniformly continuous.
	\vspace{5mm}

	\noindent
	[\textbf{(ii)}$\implies$ \textbf{(iii)}] Let $F$ be a closed set, and define
	\begin{displaymath}
		f(x) := \left(
		1 - \frac{d(x,F)}{\varepsilon}
		\right)_+.
	\end{displaymath}
	This function is uniformly continuous and bounded. Hence,
	\begin{displaymath}
		\limsup_n P_n(F) \leq \limsup_n \int f dP_n = \int f dP \leq P(B_\varepsilon (F)).
	\end{displaymath}
	Take the limit for $\varepsilon \to 0$, and conclude that $\limsup_n P_n(F) \leq P(F)$.
	\vspace{5mm}

	\noindent
	[\textbf{(iii)}$\implies$ \textbf{(iv)}] Just use $F^c$ and
	\begin{displaymath}
		\liminf_n P_n(F^c) = \liminf_n 1 - P_n(F)= 1 - \limsup_n P(F) \geq 1 - P(F) = P(F^c).
	\end{displaymath}
	\vspace{5mm}

	\noindent
	[\textbf{(iii) \& (iv)}$\implies$ \textbf{(v)}] Note that if $A$ is
	in the continuity set, then $P(\bar A) = P(\mathring A)$, but using (iii)
	and (iv)
	\begin{align*}
		P(\bar A) \geq \limsup_n P_n(\bar A) \geq \limsup P_n(A)
		 & \geq \liminf_n P_n(A)            \\
		 & \geq \liminf_n P_n(\mathring A)  \\
		 & \geq P(\mathring A) = P(\bar A).
	\end{align*}
	\vspace{5mm}

	\noindent
	[\textbf{(v)}$\implies$ \textbf{(i)}] We'll use an identity
	often proved in introductory courses of Probability, but
	which will only prove later. The identity
	is for $f\geq 0 $, then $E[f] = \int f dP = \int_0^\infty P(f>x) dx$.

	First, take a function $f:E \to \mathbb R$ that is continuous and bounded.
	Let's suppose that $f:E \to [0,1]$. Note that the following
	argument can be adapted easily for the case that $f:E \to [a,b]$
	for any $a,b \in \mathbb R$.

	We begin by using the fact that:
	\begin{displaymath}
		\int f dP = \int_0^1 P(f > x)dx.
	\end{displaymath}
	Note that
	$\partial \{y \in E: f(y) >x \} \subset \{y \in E: f(y) = x\}$.
	Also,
	\begin{displaymath}
		P(E) = P(\cup_{x \in [0,1]} \{y \in E : f(y) = x\}) =1.
	\end{displaymath}
	Since $\{y \in E : f(y) = x\}$ are disjoint set (i.e
	if $y \in \{f = x_1\}$, then $y \notin \{f = x_2\}$ for $x_1\neq x_2$),
	then there are at most an enumerable number of $x_i$ such that
	$P(\{f = x_i\}) > 0$.

	Therefore,
	\begin{displaymath}
		\int f dP_n =
		\int_{[0,1]} P_n(f > x) dx =
		\int_{[0,1]\setminus \{x_1,x_2,...\}}P_n(f > x) dx+
		\int_{\{x_1,x_2,...\}}P_n(f > x) dx.
	\end{displaymath}
	But the set $\{x_1,x_2,...\}$ has null Lebesgue measure. Hence,
	\begin{displaymath}
		\int f dP_n =
		\int_{[0,1]\setminus \{x_1,x_2,...\}}P_n(f > x) dx
	\end{displaymath}
	Also,
	note that $P(\partial \cup_{x \in [0,1] \setminus \{x_1,...\}} \{f > x\}) = 0$.
	Hence, by (v), we know that if a set is a continuity set, then
	$P_n(A) \to P(A)$. We can then use the Dominated Convergence
	Theorem to conclude that
	\begin{displaymath}
		\int f dP_n =
		\int_{[0,1]\setminus \{x_1,x_2,...\}}P_n(f > x) dx \to
		\int_{[0,1]\setminus \{x_1,x_2,...\}}P(f > x) dx =
		\int f dP.
	\end{displaymath}

\end{proof}

\begin{proposition}
	Let $(E, \mathcal B(E))$ and $(E', \mathcal B(E'))$
	be metric spaces and  $h:E \to E'$ is a continuous function.
	Suppose that $P_n$ and $P$ are probability measures
	defined on $E$, and $P_n \rightharpoonup P$.
	Then,
	\begin{equation}
		h_\# P_{n} = P_{n} \circ h^{-1} \rightharpoonup
		h_\# P = P \circ h^{-1}.
	\end{equation}
	\label{prop:contpushforward}
\end{proposition}

\begin{proof}
	Note that $h_\# P_n$ and $h_\# P$ are probability measures
	defined on $E'$. So, to prove that $h_\# P_n$
	converges weakly to $h_\# P$, we need to prove that for any
	$f \in C_b(E')$, the integrals converge. Which is true, since
	\begin{displaymath}
		\int f dP_n \circ h^{-1} = \int \underbrace{f \circ h}_{\in C_b(E')} dP_n \to
		\int f dP.
	\end{displaymath}
\end{proof}

\begin{theorem}[Continuity of Pushforward]
	Let $(E, \mathcal B(E))$ and $(E', \mathcal B(E'))$
	be metric spaces and  $h:E \to E'$ a function
	such that it's set of points of descontinuity $D_h$ has
	probability zero (i.e. $P(D_h = 0)$).
	Then,
	\begin{equation}
		h_\# P_{n} = P_{n} \circ h^{-1} \rightharpoonup
		h_\# P = P \circ h^{-1}.
	\end{equation}
	Note that this is just a stronger version of Propostion \ref{prop:contpushforward}.
\end{theorem}
\begin{proof}
	Take $F \in E'$ a closed set. Then,
	\begin{align*}
		\limsup_n (P_n\circ h^{-1})(F) & =
		\limsup_n P_n(h^{-1}(F))              \\
		                               & \leq
		\limsup_n P_n(\overline{h^{-1}(F)}) \leq
		P(\overline{h^{-1}(F)}),
	\end{align*}
	where we used (iii) of Portmanteau's Theorem \ref{thm:portmanteau} in the last inequality.

	Next, note that $\overline{h^{-1}(F)} \subset D_h \cup h^{-1}(F)$. This is true
	since, for $x \in \overline{h^{-1}(F)}$, there exists $x_n \in h^{-1}(F)$ such
	that $x_n \to x$. If $x \in D_h$, then we are done. Now, if $x \notin D_h$,
	then $x$ is a point of continuity of $h$, hence $h(x_n) \to h(x)$.
	But since $x_n \in h^{-1}(F)$, then $h(x_n) \in F$ closed,
	meaning that $h(x) \in F$, thus $x \in h^{-1}(F)$.

	Finally, since $\overline{h^{-1}(F)} \subset D_h \cup h^{-1}(F)$, then
	\begin{displaymath}
		P\left(\overline{h^{-1}(F)}\right) \leq
		P(D_h \cup h^{-1}(F)) \leq
		\underbrace{P(D_h)}_{=0} +P(h^{-1}(F)) =
		P(h^{-1}(F)).
	\end{displaymath}
	Thus, for any closed set $F \in E'$ we have
	$\limsup_n (P_n\circ h^{-1})(F) \leq P(h^{-1}(F))$, which again by
	Portmanteau \ref{thm:portmanteau} implies that
	$P_{n} \circ h^{-1} \rightharpoonup P \circ h^{-1}$.

\end{proof}

\begin{lemma}
	If $P_n$ and $P$ are such that for every subsequence
	$\forall n_k$ there exists $n_{k_j}$ such that
	$P_{n_{k_j}} \rightharpoonup P$, then
	$P_n \rightharpoonup P$.
\end{lemma}
\begin{proof}
	Suppose that $P_n$ does not converges weakly to $P$. Hence,
	there exists an $f \in C_b$ and $\varepsilon_0 > 0$ such that
	for every $N \in \mathbb N$ and there exists $n \geq N$ that
	\begin{displaymath}
		\left |
		\int f dP_n - \int f dP
		\right | \geq \varepsilon_0.
	\end{displaymath}
	But if this is true, then take a subsequence $n_k$
	such that for every $n_k$ we have
	\begin{displaymath}
		\left |
		\int f dP_{n_k} - \int f dP
		\right | \geq \varepsilon_0.
	\end{displaymath}
	Next, take a sub-subsequence $n_{k_j}$. By the hypothesis,
	we have that $P_{n_{k_j}}\rightharpoonup P$, which
	is a contradiction, since
	\begin{displaymath}
		\left |
		\int f dP_{n_{k_j}} - \int f dP
		\right | \geq \varepsilon_0.
	\end{displaymath}

\end{proof}

\subsection{Tightness of Probability Families}

The main goal of this section is to prove the celebrated Prokhorov's
Theorem, which relates the notion of compactness
with the notion of tightness. This can be quite
useful, since in many situations we can easily see that
a family of distributions is tight, but it's not clear
that they are (pre)compact.

\begin{definition}[Tightness]
	We say that a family $\{P_\lambda\}_{\lambda \in \Lambda}$ of
	probability distributions in $E$ is \textit{tight}
	if $\forall \varepsilon >0$, there exists $K \subset E$
	with $K$ compact, such that
	\begin{equation}
		\inf_{\lambda \in \Lambda} P_{\lambda}(K) \geq 1-\varepsilon.
	\end{equation}
	\label{def:tight}
\end{definition}

\begin{shaded}
	\begin{note}[Pre-Compactness]
		We say that $A \subset X$ is pre-compact
		if $\bar A = X$. This means that $A$
		is dense in $X$. And we say that $A$ is compact
		if every open cover of $A$ has a finite sub-cover.
		In metric spaces, this notion of compactness is
		equivalent to what we call sequential compactness.

		For a metric space $(X,d)$, we say that
		$A \subset X$ is sequentially compact if
		every $(x_n) \subset A$ has convergent
		subsequence $x_{n_k} \to x \in A$. We say that
		it is sequentially pre-compact if instead $x \in \bar A$.

		Consider now a family of probability
		measures $\Pi = \{P_\lambda \}_{\lambda \in \Lambda}$
		We say that $\Pi$ is sequentially pre-compact with
		respect to weak-convergence
		if every $(P_n) \in \Pi$ has a subsequence
		$P_{n_k} \rightharpoonup P \in \overline \Pi$.

		A last point to remember is the idea of totally boundedness.
		In $\mathbb R$, we know that compactness
		is equivalent to being closed and bounded. But this is
		not enough in more general spaces. Instead of boundedness,
		we need the stronger notion of totally boundedness.

		We say that a set $A$ in a metric space $(X,d)$ is totally bounded
		if for every $r > 0$, there is a collection of finite
		balls that cover $A$, i.e. for $x \in A$ there is $B_r(y)$
		such that $x \in B_r(y)$. This is another way of saying
		that there exists an $r$-net that covers
	\end{note}
\end{shaded}

\begin{theorem}[Prokhorov]
  Prokhorov's theorem contains two results that are almost one the converse of the other. 
  The first result is the main (and most useful) one:
  \begin{enumerate}[(i)]
    \item If a family $\Pi = \{P_\lambda \}_{\lambda \in \Lambda}$
	is tight, then
	$\Pi$ is sequentially pre-compact with respect to the weak convergence.
    \item Also, if $E$ is a Polish space (i.e. separable and complete metric space)
	and $\Pi$ is sequentially pre-compact with respect to weak convergence, then $\Pi$ is tight.\footnote{Note that in Prokhorov, the second
		statement is not the converse of the first, since it needs stronger hypothesis.}
  \end{enumerate}

	\label{thm:prokhorov}
\end{theorem}

\begin{proof}
  For the proof, we'll start proving (ii) and then (i).

	\vspace{3mm}

	\noindent
	\textbf{(ii)} This is the easier one in the theorem. Note that if $E$ is Polish,
	then there exists a countable dense set that covers $E$. Also, we can fix
	a diameter $\frac{1}{k}$, and define a family of open subsets with $\text{diam}(A_n^k)\leq 1/k$, such that
	$\cup_{i=1}^n A_i^k \uparrow E$.

	% $\int_{\lambda \in \Lambda} P_\lambda(\cup_i=1^{n_k} A_i^k) \geq 1 - \frac{\varepsilon}{2^{k+1}}$.
	Let's start proving that since $\cup_{i=1}^n A_i^k = B^k_n$ grows to $E$
	and our family $\Pi$ is pre-compact, then for any $\varepsilon >0$, there exists $n$ big enough
	such that	$\inf_{\lambda \in \Lambda} P_\lambda(B^k_n) \geq 1 - \varepsilon$.
	If this was not true, then there would exists $\varepsilon_0 > 0$, such that
	$\inf P_\lambda(B^k_n) < 1- \varepsilon$ for every $n$, in other words,
	there would exist a sequence $P_{n}(B^k_n)\leq 1 - \varepsilon$.

	Since the family is pre-compact, there exists a convergent subsequence
	$P_{n_j} \rightharpoonup Q$.
	But by Portmanteau \ref{thm:portmanteau},
	\begin{displaymath}
		Q(B^k_{n_j})\leq \liminf_j P_{n_j}(B^k_{n_j}) \leq 1 - \varepsilon_0,
	\end{displaymath}
	which is a contradiction, since $B^k_{n_j} \uparrow E \implies Q(B^k_{n_j}) \uparrow 1$.

	Note that the proof above works for every $k \in \mathbb N$. So we can use a
	diagonal argument. Since $\forall k, B^k_n \uparrow E$, then, for each $k$
	there exists a $n_k$ such that
	\begin{displaymath}
		\inf_{\lambda \in \Lambda}P_\lambda (B^k_{n_k}) \geq 1 - \frac{\varepsilon}{2^k} \geq 1 -\varepsilon.
	\end{displaymath}

	Now, define $B := \cap_{k \in \mathbb N} B^k_{n_k}$. Note that $B$ is pre-compact,
	since for any $r >0$, there exists a finite $r$-net, just note that for $k \in \mathbb N: 1/k < r$,
	the set $B^k_{n_k} = \cup^{n_k}_{i=1}A^k_i$ is covered by a finite union of balls with diameter $r$.
	This implies that $B$ is totally limited, thus $\overline B$ is closed and totally limited,
	which implies that $\overline B$ is compact and $B$ is pre-compact.

	Finally, for $B^c = \cup_{k \in \mathbb N} (B^k_{n_k})^c$. We have
	\begin{displaymath}
		P_\lambda(B^k_{n_k}) \geq 1-\frac{\varepsilon}{2^{k}} \iff
		1- P_\lambda(B^k_{n_k}) \leq \frac{\varepsilon}{2^{k}} \iff
		P_\lambda((B^k_{n_k})^c) \leq \frac{\varepsilon}{2^{k}}.
	\end{displaymath}
	Therefore,
	\begin{displaymath}
		P_\lambda(B^c) \leq \sum_{k=1}^\infty P_\lambda((B^k_{n_k})^c) \leq \sum_{k=1}^\infty \frac{\varepsilon}{2^k} = \varepsilon
		\implies P_\lambda(B) \geq 1 - \varepsilon.
	\end{displaymath}
	Since this is true for any $P_\lambda \in \Pi$, we conclude that
	\begin{displaymath}
		\inf_{\lambda \in \Lambda} P_\lambda(B) \geq 1 - \varepsilon.
	\end{displaymath}

	\vspace{3mm}
	\noindent
	\textbf{(i)} To prove that a tight family $\{P_\lambda\}$ is sequentially pre-compact, we need
  to show that each sequence $(P_n) \subset \{P_\lambda\}$ there is a subsequence that converges weakly to $P$.
  The hardest aspect of the proof is to find a candidate for the limiting measure $P$.

  Without loss of generality, we can assume that $E$ is separable. The reason for this is that
  since $\{P_\lambda\}$ is tight, then for $i \in \mathbb N$ we have a sequence of compact
  sets $K_i$ such that
  \begin{displaymath}
    \inf_\lambda P_\lambda(K_i) \geq 1 - \frac{1}{i} \implies
    \inf_\lambda P_\lambda(\cup_{i \in \mathbb N} K_i) \geq
    \inf_\lambda P_\lambda(\sup_{i} K_i) \geq 1.
  \end{displaymath}
  Knowing that this enumerable sequence covers all the space in measure, we can   
  cover each $K_i$ and obtain a finite sub-cover of open sets, and thus, construct a sequence
  of enumerable open sets that covers $E$.

  The rest of this proof is from \citet{chin2019new}.

  % Moving on. Let's construct the following family
  % \begin{displaymath}
  %  \mathcal A := \{
  %    A^G_x : \ x \in E, G \in E \text{ open, with } x \in G \text{ such that }
  %    A^G_x \text{ is open and } x \in A^G_x \subset \overline{A}^G_x \subset G
  %  \},
  % \end{displaymath}
  % which is the family of open sets of $E$ with the closure still inside another open set of $E$.
  % It can be shown that such family is well-defined and is enumerable due to the assumption of separability of $E$.

\end{proof}


% In the proof of Prokhorov's Theorem \ref{thm:prokhorov}, the famous Kolmogorov's Extension Theorem
% was used.
The following proof is from \citet{impaproba}.
\begin{theorem}[Kolmogorov's Extension Theorem - Simpler Version]
  For $n \in \mathbb N$, let $P_n \in \mathcal P(\mathbb R^n)$ such that
  \begin{displaymath}
    P_{n+1}  (A\times \mathbb R) = P_n(A), \quad \forall \ A \in \mathcal B(\mathbb R^n).
  \end{displaymath}
  Therefore, there exists a unique probability measure $P$ in $(\mathbb R^{\mathbb N}, \mathcal B(\mathbb R^\mathbb N))$
  such that $P(A\times \mathbb R \times ...) = P_n(A)$ for every $n$ and every $A \in \mathbb R^n$.
\end{theorem}


